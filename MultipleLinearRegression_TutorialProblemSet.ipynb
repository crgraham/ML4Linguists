{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246bb1f9",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression: A Tutorial\n",
    "\n",
    "Author: K. Voudouris (2022)\n",
    "\n",
    "In this tutorial, we will be extending our intuitions about linear regression to cases where we have several predictor variables.\n",
    "\n",
    "First, let's import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9283d",
   "metadata": {},
   "source": [
    "Next we import the Iris dataset. The Iris dataset is the classic dataset for Data Science.It contains five columns about Iris flowers: Petal Length, Petal Width, Sepal Length, Sepal Width, and Class (species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "iris_data = pd.read_csv(\"./iris_csv.csv\")\n",
    "iris_data.head() #show the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff465c",
   "metadata": {},
   "source": [
    "## Introducing Multiple Linear Regression\n",
    "\n",
    "The previous tutorial explored linear regression with one predictor $\\mathbf{x}$ and one predicted variable $\\mathbf{y}$. We explored how to regress one (predicted) variable onto one (predictor) variable, allowing us to determine how one unit change in the predictor variable changes the value of the predicted variable. To do this, we need to estimate the gradient, and in the affine case, the y-intercept. However, very often we want to look at how several predictor variables predict some other, predicted, variable. We use *multiple linear regression* for this.\n",
    "\n",
    "A few terminological and notational points first:\n",
    "- *Multiple* linear regression is slightly different from *multivariate* linear regression. Multiple regression tends to refer to the case where there is only one predicted variable, $\\mathbf{y}$. Multivariate regression tends to refer to the case where there are several predicted variables, $\\mathbf{Y}$.\n",
    "- When we are denoting variables with letter symbols, there are some conventions inherited from linear algebra. Unbolded lower-case letters, like *x*, refer to *scalars*. These are single values (they have a magnitude but not a direction). Bolded lower-case letters, like $\\mathbf{x}$, refer to **vectors**. You can think of these as a single list of values, usually a column from a dataframe. They have a magnitude and a direction, and can be used to characterise points in a space. You can think of a vector as an instruction set to arrive at a point in space: move n units along the x-axis, then m units along the y-axis, then q units along the z-axis, etc. Bolded upper-case letters, like $\\mathbf{X}$, refer to **MATRICES**. You can think of these as several lists of values, or several vectors side-by-side. For programming purposes, it is usually a whole (subset of a) dataframe.\n",
    "\n",
    "In multiple linear regression, we use a set/matrix of predictor variables, $\\mathbf{X}$, to predict a predictor variable, $\\mathbf{y}$.\n",
    "\n",
    "This results in a model representation, where we predict a scalar *y* value given a vector of *x* values:\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "\n",
    "You might also see this written in terms of hypothesis functions. \n",
    "\n",
    "$$h_{\\beta}(\\mathbf{x}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "\n",
    "In other words, a function given a vector of x-values as input, returns a hypothesis, a scalar y-value.\n",
    "\n",
    "We use a labelled dataset {$\\mathbf{X}, \\mathbf{y}$} to fit this model/hypothesis function. This makes it a kind of supervised learning technique, since every set of x-values has a corresponding y-value.\n",
    "\n",
    "Let's try to get some intuition for why this works. Let's plot 2 predictor variables and 1 predicted variable. Let's go with SepalLength and SepalWidth predicting PetalLength. You can interact with the graph using your cursor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "############Fill in the parts of the code necessary\n",
    "\n",
    "x1 = iris_data[''].values\n",
    "x2 = iris_data[''].values\n",
    "y = iris_data[''].values\n",
    "\n",
    "###########Plot the data using matplotlib\n",
    "\n",
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1, x2, y, 'gray')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('x1', fontsize = 15) \n",
    "ax.set_ylabel('x2', fontsize = 15)\n",
    "ax.set_zlabel('y', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532d700",
   "metadata": {},
   "source": [
    "There seems to be some kind of linear relationship here. You might notice that there are two 'clusters' of data. This might be because of species. Explore the data frame to see if that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['class'] = iris_data['class'].astype('category') # this line of code just changes the `class` column into a category/factor\n",
    "\n",
    "# Find the unique values in the class column using the pandas function `unique()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataframe to include only a subset of the species, and then plot it to see if you have eliminated the right one\n",
    "# Use Google if you cannot find the right syntax!\n",
    "\n",
    "iris_data_subset = \n",
    "\n",
    "print(iris_data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the parts of the code necessary. Make sure to keep the same variables: \n",
    "# x1 is Sepal Length, x2 is Sepal Width, y is Petal Length\n",
    "\n",
    "# adapt the code from above to make the right variables\n",
    "x1_subset = \n",
    "x2_subset = \n",
    "y_subset = \n",
    "\n",
    "###########Plot the data using matplotlib\n",
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1_subset, x2_subset, y_subset, 'gray')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('x1', fontsize = 15) \n",
    "ax.set_ylabel('x2', fontsize = 15)\n",
    "ax.set_zlabel('y', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fdee5",
   "metadata": {},
   "source": [
    "Once we have eliminated one of the species, there seems to be a nice linear relationship, right?\n",
    "\n",
    "Whereas in the 2D case, we wanted to plot a 1D line-of-best fit to describe this tendency, this wouldn't be a good characterisation of this case. If we just plotted a line on here, it would at most describe a relationship between two variables at a time. What we need is 2D line, called a *plane*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85995be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1val, x2val = np.meshgrid(range(4,9), range(2,5)) # this line just creates some x values to create a plane in the visualisation\n",
    "\n",
    "## Change these parameters:\n",
    "\n",
    "b0 = 0 #where does the plane intercept the y-axis?\n",
    "b1 = 0 #what's the gradient coefficient for the x1-y plane?\n",
    "b2 = 0 #what's the gradient coefficient for the x2-y plane?\n",
    "\n",
    "# calculate corresponding z\n",
    "yval = b0 + b1*x1val + b2*x2val #create vector of yvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d4ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "# Plot the data with a hyperplane. It's an interactive plot, move it around!\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1_subset, x2_subset, y_subset, 'gray')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('x1', fontsize = 15) #change the axis labels as necessary\n",
    "ax.set_ylabel('x2', fontsize = 15)\n",
    "ax.set_zlabel('y', fontsize = 15)\n",
    "ax.plot_surface(x1val, x2val, yval, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6a6f6",
   "metadata": {},
   "source": [
    "Instead of finding a line-of-best-fit, we are trying to find a \"plane-of-best-fit\". A flat surface that characterises the relationship between our variables. \n",
    "\n",
    "What are the `b0`, `b1`, `b2` parameters doing here? `b0` ($\\beta_{0}$) is the y-intercept, it tells you what $\\mathbf{y}$ is when $\\mathbf{x}_{1}$ *and* $\\mathbf{x}_{2}$ are equal to 0. `b1` ($\\beta_{1}$) tells you how $\\mathbf{y}$ changes when $\\mathbf{x}_{1}$ is held constant (and the y-intercept too, but that is constant by definition). It just becomes a line-of-best-fit relating $\\mathbf{x}_{1}$ and $\\mathbf{y}$. The same line of thinking applies to `b2` ($\\beta_{2}$).\n",
    "\n",
    "So `b0` is the y-intercept when $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ are equal to 0, and `b1` and `b2` are the gradient coefficients telling us how $\\mathbf{y}$ when we hold the other predictor variables constant. \n",
    "\n",
    "Another way to think about it is in terms of a set of instructions. Move $\\beta_{1}$ units along the $\\mathbf{x}_{1}$ axis, then $\\beta_{2}$ units along the $\\mathbf{x}_{2}$ axis, and finally move $\\beta_{0}$ units up the y-axis. We can add these instructions together because of the distributivity and additivity of vectors. See [this tutorial](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces) on linear algebra for more.\n",
    "\n",
    "We are starting to get an intuition for why we can generalise simple linear regression to multiple linear regression:\n",
    "\n",
    "$$\\mathbf{y} = \\beta_{0} + \\beta_{1}\\mathbf{x}_{1} + \\beta_{2}\\mathbf{x}_{2} + ... + \\beta_{n}\\mathbf{x}_{n}$$\n",
    "\n",
    "which we can rearrange easily to:\n",
    "\n",
    "$$\\mathbf{y} = \\beta_{1}\\mathbf{x}_{1} + \\beta_{2}\\mathbf{x}_{2} + ... + \\beta_{n}\\mathbf{x}_{n} + \\beta_{0}$$\n",
    "\n",
    "You can think of this equation as a set of instructions to arrive at a point in your n-dimensional space. It says move $\\beta_{1}$ units along the $\\mathbf{x}_{1}$ axis, then $\\beta_{2}$ units along the $\\mathbf{x}_{2}$ axis, ..., then $\\beta_{n}$ units along the $\\mathbf{x}_{n}$ axis, and finally $\\beta_{0}$ units up the $\\mathbf{y}$ axis. Linear algebra allows you to simply concatenate those instructions together by adding them, providing you with an instruction set to navigate your n-dimensional space and move from the origin to any $\\mathbf{y}$ you want, depending on values of ${x_{1}, ...x_{n}}$. The resulting equation defines a plane in n-dimensional space, which is called a *hyperplane*. This is super useful for predicting things: if you know the the values of ${x_{1}, ...x_{n}}$, your equation with its beta values provides you with a complete instruction set to move from the origin to a particular y-value, enabling you to make a prediction based on some data you have collected. \n",
    "\n",
    "Our job, or rather the machine's, is to find the values of ${\\beta_{1}, ...\\beta_{n}}$ that characterise the \"hyperplane-of-best-fit\" for the data given by {$\\mathbf{X, y}$}.\n",
    "\n",
    "Finding this \"hyperplane-of-best-fit\" is very often the goal of Machine Learning algorithms. When we say we are fitting a model, we are finding this hyperplane-of-best-fit. The hyperplane is our *model* or *model representation*, which we then use to generalise to populations or to make predictions based on novel data. As we move to more complex models, we might not want to fit 'flat' or 'linear' hyperplanes. They might not be very good model representations of the data. This nonlinearity is where modern ML algorithms have their benefit over older statistical approaches like linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd42711",
   "metadata": {},
   "source": [
    "## Introducing the Cost Function\n",
    "\n",
    "For simple linear regression with just one predictor and one predicted variable, we saw two metrics for quantifying how \"well\" a line-of-best-fit characterises the dataset. These were Root Mean Squared Error (RMSE) and $R^2$. These are part of a set of techniques that measure \"cost\". How much does it \"cost\" us to replace the data with a line-of-best-fit? How much \"information\" do we \"lose\"? If the line-of-best-fit poorly represents the data, that costs us a lot (in terms of predictive accuracy or generalisability), and we can see this because there is a high RMSE or a low $R^2$. If, however, it perfectly represents the data, then replacing the data with the line-of-best-fit costs us *nothing*.\n",
    "\n",
    "RMSE and $R^2$ are kinds of *cost function* (note that they are sometimes called *loss* functions too). Cost functions take as input the model representation (the $\\beta$ parameters of the line/(hyper)plane-of-best-fit) return a cost as output, the numerical value representing the degree of error between model and data. In this tutorial, we will be focusing on RMSE and its variants because of the limited utility of $R^2$ for machine learning (see [here](https://jonlefcheck.net/2013/03/13/r2-for-linear-mixed-effects-models/) and [here](https://data.library.virginia.edu/is-r-squared-useless/)). \n",
    "\n",
    "RMSE:\n",
    "$$J(model) = \\sqrt{\\sum\\limits _{j = 1} ^{m}\\frac{1}{m}(\\hat{y}_{j}-y_{j})^2}$$\n",
    "\n",
    "Since our model representation is characterised by the $\\beta$ parameters of the line/(hyper)plane-of-best-fit, we can replace *model* with those parameters:\n",
    "\n",
    "RMSE:\n",
    "$$J(\\{\\beta_{0}, ..., \\beta_{n}\\}) = \\sqrt{\\sum\\limits _{j = 1} ^{m}\\frac{1}{m}(\\hat{y}_{j}-y_{j})^2}$$\n",
    "\n",
    "In more compact notation, we can represent the $\\beta$ parameters as a vector **$\\mathbf{\\beta}$** (in bold-face):\n",
    "\n",
    "RMSE:\n",
    "$$J(\\mathbf{\\beta}) = \\sqrt{\\sum\\limits _{j = 1} ^{m}\\frac{1}{m}(\\hat{y}_{j}-y_{j})^2}$$\n",
    "\n",
    "There is one final set of changes we can make to RMSE. Instead of $\\frac{1}{m}$, we use $\\frac{1}{2m}$. We also move this fraction outside of the sum.\n",
    "\n",
    "RMSE:\n",
    "$$J(\\mathbf{\\beta}) = \\sqrt{\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2}$$\n",
    "\n",
    "Finally, we forget about square rooting. This is the Mean Squared Error:\n",
    "MSE:\n",
    "$$J(\\mathbf{\\beta}) = \\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2$$\n",
    "\n",
    "This is to make finding the partial derivative (see later) easier. It doesn't actually alter the behaviour of RMSE, since it just scales it down by a constant of 2 and scales it up by a constant of 'not-squaring' it. Thus, it does not affect where the minima are. The reason it hasn't been introduced this way before is simply because the previous version we used is more intuitive when getting to grips with regression in the first instance. Importantly, RMSE, MSE, and their scaled variants, are all cost functions.\n",
    "\n",
    "NB: You might be wondering: surely the cost function takes the data, {$\\mathbf{X, y}$}, as input too? How can you calculate the RMSE, for instance, without the data to compute $\\hat{y}_{j}$ and $y_{j}$? The reason these aren't included in the cost function notation is that they are constant. The cost function maps changing $\\mathbf{\\beta}$ values to an error quantity (like RMSE and MSE). It's only interested in change, and the data are a constant across the computation, so we don't need to include them in the cost function notation.\n",
    "\n",
    "Since RMSE is a measure of the error between the observed data and the model-predicted data, it generalises as a cost function from simple cases with just one predictor variable to complex cases with *n* predictor variables (i.e., multiple linear regression). Explore this idea with the next code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c84fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1val, x2val = np.meshgrid(range(4,9), range(2,5)) # this line just creates some x values to create a plane in the visualisation\n",
    "\n",
    "## Change these parameters:\n",
    "\n",
    "b0 = 0 #where does the plane intercept the y-axis?\n",
    "b1 = 0 #what's the gradient coefficient for the x1-y plane?\n",
    "b2 = 0 #what's the gradient coefficient for the x2-y plane?\n",
    "\n",
    "############# DON'T CHANGE THIS CODE ##################### \n",
    "\n",
    "# calculate corresponding z\n",
    "yval = b0 + b1*x1val + b2*x2val #create vector of yvalues\n",
    "\n",
    "# model yvalues based on data and model\n",
    "\n",
    "yval2 = b0 + b1*x1_subset + b2*x2_subset\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1_subset, x2_subset, y_subset, 'gray')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('x1', fontsize = 15) \n",
    "ax.set_ylabel('x2', fontsize = 15)\n",
    "ax.set_zlabel('y', fontsize = 15)\n",
    "ax.plot_surface(x1val, x2val, yval, alpha=0.2)\n",
    "for i in range(0,(len(x1_subset)-1)): # plot the differences between the plane and the data\n",
    "    ax.plot([x1_subset[i],x1_subset[i]], [x2_subset[i],x2_subset[i]], [y_subset[i],yval2[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a42d0",
   "metadata": {},
   "source": [
    "Machine learning people often talk about *optimisation* and *optimisation problems*. These most usually involve minimising or maximising a function. In our case, we want to minimise a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd135d96",
   "metadata": {},
   "source": [
    "## Minimising the Cost Function\n",
    "\n",
    "What does it mean to minimise the cost function? In essence, it means that we want to find the values of the vector $\\mathbf{\\beta}$ that result in the smallest cost. We do this by finding (one of) the minimum(/minima) of the cost function. It's easiest to think about this graphically, so let's build to that. We will have to stick with two $\\beta$ values, because that is the most we can represent graphically. Using MSE to solve multiple linear regression problems is part of **(ordinary) least squares** regression techniques, quite simply because we are trying to find the solution that has the **least squared error**.\n",
    "\n",
    "Build a function that calculates MSE:\n",
    "\n",
    "$$J(\\mathbf{\\beta}) = \\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_fun(beta0, beta1, datax, datay): #inputs are the beta values and the x and y values needed to compute y-hat and y\n",
    "    beta0, beta1 = np.array(beta0), np.array(beta1)\n",
    "    ## enter code here\n",
    "    # Just leave comments and variable names\n",
    "    yhat =                  # first calculate what the model would predict given the values in datax\n",
    "    m =                     # number of datapoints, use the `len()` function to calculate length of datax or datay (it doesn't matter, they should be the same)\n",
    "    difference_vec =        # calculate the difference between yhat and y. You can simply subtract one vector from the other\n",
    "    sq_diff_vec =           # square the difference vector you've just produced\n",
    "    sum_diffs =             # sum the values in the squared vector using the sum() function\n",
    "    mse =                   # calculate MSE by dividing by 2m\n",
    "    return(mse)\n",
    "\n",
    "print(MSE_fun(0, 0, x1_subset, y_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb535e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "# Compare it to the output from the scikit-learn function:\n",
    "beta0 = 0\n",
    "beta1 = 0\n",
    "\n",
    "y_predicted = beta0 + beta1*x1_subset #the model prediction for each value of Sepal Length in the dataset\n",
    "\n",
    "print(\"MSE not scaled. Scikit-learn\", mean_squared_error(y_subset, y_predicted))\n",
    "## Notice it is twice the value we got above, because above we have scaled it by half\n",
    "print(\"MSE scaled by half. Scikit-learn\", mean_squared_error(y_subset, y_predicted)/2) #small differences due to precision with small numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdd26a",
   "metadata": {},
   "source": [
    "Let's plot a range of beta values and their corresponding RMSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398a902",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE ##################### \n",
    "\n",
    "beta0, beta1 = np.meshgrid(np.linspace(-3,5,100), np.linspace(-1,2,100)) # create a bunch of values that define a grid surface\n",
    "beta0 = np.ndarray.flatten(beta0) #flatten them to put in a dataframe\n",
    "beta1 = np.ndarray.flatten(beta1)\n",
    "\n",
    "mse_df = {\n",
    "    'beta0': beta0,\n",
    "    'beta1': beta1} # make a dictionary\n",
    "mse_df = pd.DataFrame(mse_df) #convert it to a dataframe\n",
    "\n",
    "mse_df['MSE'] = mse_df.apply(lambda row : MSE_fun(row['beta0'], row['beta1'], x1_subset, y_subset), axis = 1) # this function applies your function to every beta0 and beta1 value\n",
    "\n",
    "print(mse_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44623e9a",
   "metadata": {},
   "source": [
    "Now let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('beta0', fontsize = 15) #change the axis labels as necessary\n",
    "ax.set_ylabel('beta1', fontsize = 15)\n",
    "ax.set_zlabel('MSE (J(beta))', fontsize = 15)\n",
    "ax.scatter(mse_df['beta0'], mse_df['beta1'], mse_df['MSE'], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd184737",
   "metadata": {},
   "source": [
    "There's a valley in the cost function, where cost is minimised. There are a range of beta values that would provide you with a low cost. This is very often the case in machine learning. This can be because of the phenomenon above where the cost function defines a valley. It can also be because there are several minima that could have equally low costs (although not in linear regression). It underscores the idea that we should seek as large a dataset as possible to train our algorithms. We could add loads more iris data and it might tell us that there isn't actually a uniform 'valley' in the cost function but instead a 'bowl'. More data means we can be more sure of the predictive value of our model.\n",
    "\n",
    "Of course, we can't visualise the cost functions for multiple linear regression with more than two beta parameters. But the cost function can be defined in an n-dimensional space, and the optimisation problem is the same: find the (local) minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e49dc",
   "metadata": {},
   "source": [
    "## Introducing Gradient Descent\n",
    "\n",
    "We have seen what a cost function is and got a sense for how we would use it to find beta parameter values that best characterise the data, and therefore arrive at a model representation. However, how does a machine arrive at these beta values by itself? This is where optimisation algorithms come in. A machine can *learn* the best model representation for a set of data by minimising the cost function (in lots of cases). It finds the local minimum in the cost function using an algorithm called **gradient descent**.\n",
    "\n",
    "The intuition for gradient descent is very simple: \n",
    "1. Randomly pick some beta parameter values and calculate the cost, $J(\\beta)$. \n",
    "2. Calculate the gradient at that point, and then move a little bit in a downward direction. \n",
    "3. Update the beta values at the newly arrived at point.\n",
    "4. Repeat steps 2 and 3 several times, until $J(\\beta)$ stabilises around a value. This means the algorithm has *converged*.\n",
    "\n",
    "Look at the next plot to visualise this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db90c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "red_df = mse_df.iloc[70::100, :] #dataframe of highlighted points\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('beta0', fontsize = 15) #change the axis labels as necessary\n",
    "ax.set_ylabel('beta1', fontsize = 15)\n",
    "ax.set_zlabel('MSE (J(beta))', fontsize = 15)\n",
    "ax.scatter(mse_df['beta0'], mse_df['beta1'], mse_df['MSE'], alpha=0.2)\n",
    "ax.scatter(red_df['beta0'], red_df['beta1'], red_df['MSE'], color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6ea2b",
   "metadata": {},
   "source": [
    "But how does the algorithm calculate the gradient at a point in the space? It does this using *differentiation*, partial differentiation to be precise. Differentiating (usually) allows you to find the gradient, or slope, at a point on the graph. A partial derivative just means finding the gradient with respect to just one of your predictor variables at a time. For example, you find the partial derivative of the cost function with respect to $\\beta_{0}$, ignoring $\\beta_{1}$. We represent that as follows:\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1})$$\n",
    "This represents the gradient at a random point for $\\beta_{0}$ and $\\beta_{1}$, giving us a quantity for how 'slopey' it is. There are many approaches for calculating (or at least approximating) partial derivatives (see this [tutorial](https://towardsdatascience.com/a-quick-introduction-to-derivatives-for-machine-learning-people-3cd913c5cf33)). For now, all we need to know is that computers are quite good at calculating partial derivatives, which is good for machine learning generally and gradient descent specifically!\n",
    "\n",
    "The algorithm for gradient descent thus far looks like this, then:\n",
    "\n",
    "1. Pick random values for $\\beta_{0}$ and $\\beta{1}$\n",
    "2. Calculate the partial derivatives of $J(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{0}$ and $\\beta{1}$ to provide you with the gradient at those points in the $\\beta_{0}$ and $\\beta{1}$ directions.\n",
    "3. Move down the slope a little bit.\n",
    "4. Update values for $\\beta_{0}$ and $\\beta{1}$.\n",
    "5. Repeat steps 2-4 until convergence\n",
    "\n",
    "How do we implement step 3? We want to update $\\beta_{0}$ and $\\beta{1}$ such that $J(\\beta_{0}, \\beta_{1})$ is smaller. We simply subtract the partial derivative (the gradient) from $\\beta_{0}$ and $\\beta{1}$ respectively:\n",
    "\n",
    "$$\\beta_{0} := \\beta_{0} - \\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1})$$\n",
    "$$\\beta_{1} := \\beta_{1} - \\frac{\\partial}{\\partial (\\beta_{1})}J(\\beta_{0}, \\beta_{1})$$\n",
    "\n",
    "where you read ':=' as 'update'. We are updating our $\\beta$ values by moving them down the slope. Let's see how this works by using some real numbers. \n",
    "1. Random starting values: $\\beta_{0} = 0$ and $\\beta_{1} = 0$ (most often we start at the origin)\n",
    "2. The gradients with respect to $\\beta_{0}$ and $\\beta_{1}$ respectively is as follows:\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1}) = 0.5$$\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1}) = -0.5$$\n",
    "    Meaning that a change of +1 unit in the $\\beta_{0}$ direction results in a change of +0.5 in cost, and a change of +1 unit in the $\\beta_{1}$ direction results in a change of -0.5 in cost.\n",
    "3. Now we update the beta values:\n",
    "$$\\beta_{0} = 0 - 0.5 = -0.5$$\n",
    "$$\\beta_{1} = 0 - -0.5 = 0.5$$\n",
    "4. We then repeat this, using the output of the last update, recalculating the gradient. Say they are now 0.25 and 0 respectively:\n",
    "$$\\beta_{0} = -0.5 - 0.25 = -0.75$$\n",
    "$$\\beta_{1} = 0.5 - 0 = 0.5$$\n",
    "5. Repeat until convergence.\n",
    "\n",
    "Now, if the gradient is very steep, this can mean that the 'step' between the last $\\beta$ and the next $\\beta$ is big, maybe even overstepping the local minimum. Alternatively, if the gradient is really shallow, it might take forever to get to the local minimum, wasting resources. We can use a 'learning rate' parameter, $\\alpha$, to modulate this, making the steps smaller or larger.\n",
    "\n",
    "$$\\beta_{0} := \\beta_{0} - \\alpha \\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1})$$\n",
    "$$\\beta_{1} := \\beta_{1} - \\alpha \\frac{\\partial}{\\partial (\\beta_{1})}J(\\beta_{0}, \\beta_{1})$$\n",
    "\n",
    "Our algorithm now looks like this:\n",
    "1. Initialisation: random values for $\\beta_{0}$ and $\\beta_{1}$\n",
    "2. Iterative Update: User-defined learning rate $\\alpha$\n",
    "$$\\beta_{0} := \\beta_{0} - \\alpha \\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1})$$\n",
    "$$\\beta_{1} := \\beta_{1} - \\alpha \\frac{\\partial}{\\partial (\\beta_{1})}J(\\beta_{0}, \\beta_{1})$$\n",
    "3. Convergence: stop when $\\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, \\beta_{1})$ and $\\frac{\\partial}{\\partial (\\beta_{1})}J(\\beta_{0}, \\beta_{1})$ are (roughly) equal to 0 (gradient at a minimum is going to be 0). Alternatively (and more efficiently), stop when the change in beta values is very small.\n",
    "\n",
    "This algorithm generalises to multiple linear where you have {$\\beta_{0}, ..., \\beta_{n}$}:\n",
    "1. Initialisation: random values for {$\\beta_{0}, ..., \\beta_{n}$}.\n",
    "2. Iterative Update: User-defined learning rate $\\alpha$\n",
    "$$\\beta_{0} := \\beta_{0} - \\alpha \\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, ..., \\beta_{n})$$\n",
    "$$...$$\n",
    "$$\\beta_{n} := \\beta_{n} - \\alpha \\frac{\\partial}{\\partial (\\beta_{n})}J(\\beta_{0}, ..., \\beta_{n})$$\n",
    "3. Convergence: stop when $\\frac{\\partial}{\\partial (\\beta_{0})}J(\\beta_{0}, ..., \\beta_{n}), ..., \\frac{\\partial}{\\partial (\\beta_{n})}J(\\beta_{0}, ..., \\beta_{n})$ are (roughly) equal to 0, OR the step changes are very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8a15f",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent With MSE\n",
    "\n",
    "With our gradient descent algorithm in hand, let's walk through how we implement it with a Mean Squared Error cost function. Recall that MSE is defined as follows:\n",
    "\n",
    "$$J(\\mathbf{\\beta}) = \\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2$$\n",
    "\n",
    "Let's replace all mentions of $J(\\mathbf{\\beta})$ in our algorithm above with our Mean Squared Error equation:\n",
    "\n",
    "1. Initialisation: random values for {$\\beta_{0}, ..., \\beta_{n}$}.\n",
    "2. Iterative Update: User-defined learning rate $\\alpha$\n",
    "$$\\beta_{0} := \\beta_{0} - \\alpha \\frac{\\partial}{\\partial (\\beta_{0})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2$$\n",
    "$$...$$\n",
    "$$\\beta_{n} := \\beta_{n} - \\alpha \\frac{\\partial}{\\partial (\\beta_{n})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2$$\n",
    "3. Convergence: stop when $\\frac{\\partial}{\\partial (\\beta_{0})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2, ..., \\frac{\\partial}{\\partial (\\beta_{n})}J(\\beta_{0}, ..., \\beta_{n})$ are (roughly) equal to 0 OR the changes in beta values is very small.\n",
    "\n",
    "The way we have set up MSE makes finding the partial derivative very straightforward. We can simplify it as follows:\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{0})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2 \\equiv \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})$$\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{1})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2 \\equiv \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}[(\\hat{y}_{j}-y_{j})x_{j}^1]$$\n",
    "$$...$$\n",
    "$$\\frac{\\partial}{\\partial (\\beta_{n})}\\frac{1}{2m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})^2 \\equiv \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}[(\\hat{y}_{j}-y_{j})x_{j}^n]$$\n",
    "\n",
    "Notice how the squaring has gone and we are only multiplying by $\\frac{1}{m}$. Notice also that for $\\beta_{1, ..., n}$, we are introducing the x variable into the summation. We needn't worry to much about why this is, but see [here](https://sebastianraschka.com/faq/docs/mse-derivative.html#:~:text=What%20is%20the%20derivative%20of%20the%20Mean%20Squared,%E2%88%91%20i%20%3D%201%20n%20%28%20t%20) if you are interested.\n",
    "\n",
    "We now have a very easy-to-compute gradient descent algorithm:\n",
    "1. Initialisation: random values for {$\\beta_{0}, ..., \\beta_{n}$}.\n",
    "2. Iterative Update: User-defined learning rate $\\alpha$\n",
    "$$\\beta_{0} := \\beta_{0} - \\alpha \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j})$$\n",
    "$$\\beta_{1} := \\beta_{1} - \\alpha \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}[(\\hat{y}_{j}-y_{j})x_{j}^1]$$\n",
    "$$...$$\n",
    "$$\\beta_{n} := \\beta_{n} - \\alpha \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}[(\\hat{y}_{j}-y_{j})x_{j}^n]$$\n",
    "3. Convergence: stop when {$\\frac{1}{m}\\sum\\limits _{j = 1} ^{m}(\\hat{y}_{j}-y_{j}), ...,  \\frac{1}{m}\\sum\\limits _{j = 1} ^{m}[(\\hat{y}_{j}-y_{j})x_{j}^n]$} are (roughly) equal to 0 OR changes are very small.\n",
    "\n",
    "Let's try to implement this for the simple case with just `beta0` and `beta1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First write a function that calculates everything after alpha for beta0 (the partial derivative with respect to beta0)\n",
    "\n",
    "def beta0_partialder(beta0, beta1, datax, datay): #inputs are the beta values and the x and y values needed to compute y-hat and y\n",
    "    beta0, beta1 = np.array(beta0), np.array(beta1)\n",
    "    ## enter code here\n",
    "    yhat =                  # first calculate what the model would predict given the values in datax\n",
    "    m =                     # number of datapoints using len() function\n",
    "    difference_vec =        # difference between yhat and true y values, simply subtract the vectors from each other\n",
    "    sumdiffvec =            # sum the vector values\n",
    "    beta0_partialder =      # divide by m\n",
    "    return(beta0_partialder)\n",
    "\n",
    "print(beta0_partialder(0.5, 0.5, x1_subset, y_subset)) # if your function is working properly, you should get around -1.275 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write a function that calculates the partial derivative for beta1\n",
    "\n",
    "def beta1_partialder(beta0, beta1, datax, datay): #inputs are the beta values and the x and y values needed to compute y-hat and y\n",
    "    beta0, beta1 = np.array(beta0), np.array(beta1)\n",
    "    ## enter code here\n",
    "    # Just leave comments and variable names\n",
    "    yhat =                    # first calculate what the model would predict given the values in datax\n",
    "    m =                       # len() function\n",
    "    difference_vec =          # difference vector\n",
    "    diffvec_x =               # multiply difference vector by x vector\n",
    "    sumdiffvec =              # sum the output of diffvec_x\n",
    "    beta1_partialder =        # divide by m\n",
    "    return(beta1_partialder)\n",
    "\n",
    "print(beta1_partialder(0.5, 0.5, x1_subset, y_subset)) # if your function is working properly, you should get around -8.215 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use this while loop to iteratively update beta0 and beta1 until convergence\n",
    "\n",
    "BETA0, BETA1 = -1, 0.5 \n",
    "alpha = 0.001 #step size - play around with this - if it's too large, it doesn't converge!\n",
    "precision = 0.0000000001 #when to stop algorithm? How different to current and previous values have to be before you stop?\n",
    "max_iterations = 10000 #how many iterations to go through before stopping?\n",
    "previous_step_size_BETA0 = 1\n",
    "previous_step_size_BETA1 = 1\n",
    "iterations = 0\n",
    "\n",
    "############# DON'T CHANGE THIS CODE #####################\n",
    "\n",
    "while previous_step_size_BETA0 > precision and previous_step_size_BETA1 > precision and iterations < max_iterations:\n",
    "    iterations = iterations + 1\n",
    "    prevBETA0 = BETA0 # store current value as previous value\n",
    "    prevBETA1 = BETA1 # store current value as previous value\n",
    "    BETA0 = prevBETA0 - alpha*beta0_partialder(prevBETA0, prevBETA1, x1_subset, y_subset)\n",
    "    BETA1 = prevBETA1 - alpha*beta1_partialder(prevBETA0, prevBETA1, x1_subset, y_subset)\n",
    "    previous_step_size_BETA0 = abs(BETA0 - prevBETA0)\n",
    "    previous_step_size_BETA1 = abs(BETA1 - prevBETA1)\n",
    "    print(\"Iteration\", iterations, \"\\nBeta0 value is\", BETA0, \"\\nBeta1 value is\", BETA1)\n",
    "\n",
    "print(\"Converged values are\\n\", BETA0, BETA1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa506c8",
   "metadata": {},
   "source": [
    "Well done for implementing gradient descent! It's probably *the* most important algorithm to know in Machine Learning (don't quote me though...). Play around with the parameters above to get a sense for how the code works and what the parameters do.\n",
    "\n",
    "Once we have converged on some beta parameters, we simply plug those back into the model representation:\n",
    "\n",
    "$$y = -0.995 + 0.943x_{1}$$\n",
    "$$h_{-0.995,0.943}(x_{1}) = -0.995 + 0.943x_{1}$$\n",
    "\n",
    "Now, when we are given novel predictor data, we can make a prediction about what we expect the output to be. In our case, we can predict Petal Length from any given Sepal Length in 2 out of 3 species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa2d82",
   "metadata": {},
   "source": [
    "## Implementing Regression Easily - BONUS SECTION\n",
    "\n",
    "Luckily, you don't have to do any of the above when you implement a linear regression in practice. The first point to make is that optimising the parameters for linear regression often doesn't require an iterative algorithm like gradient descent. As was mentioned in the previous tutorial, we can use the *normal equations* to find the minimum of the cost function straight away, without having to iterate through a bunch of possible values. To do this, we are going to explore a handy trick in programming called *vectorisation*.\n",
    "\n",
    "### Vectorisation\n",
    "\n",
    "Recall that the model representation for multiple linear regression is as follows:\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "\n",
    "The notation I presented at the start means that we read this as:\n",
    "A *scalar* y value equals a *scalar* $\\beta_{0}$ plus a *scalar* beta multiplied by a *scalar* $x_{1}$ value, up to $x_{n}$.\n",
    "\n",
    "We can simplify this down much more easily if we add in an $x_{0}$:\n",
    "$$y = \\beta_{0}x_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "which is the same as:\n",
    "$$y = \\mathbf{β}^{T}\\mathbf{x}$$\n",
    "\n",
    "The little T means that the vector is transposed. Fleshing it out, it looks like this:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "\\beta_{0} & ... & \\beta_{n}\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cc} \n",
    "x_{0}\\\\ \n",
    "...\\\\\n",
    "x_{n}\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Matrix multiplication tells us that a 1 x n matrix multiplied by an n x 1 matrix is a 1 x 1 matrix. In other words, a scalar, which is what *y* is.\n",
    "\n",
    "Of course, we aren't collecting a predictor variable $x_{0}$, so we make that always equal to 1, so it doesn't change the $\\beta_{0}$ value, the y-intercept:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "\\beta_{0} & \\beta_{1} & ... & \\beta_{n}\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cc} \n",
    "1\\\\\n",
    "x_{1}\\\\\n",
    "...\\\\\n",
    "x_{n}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Framing the model representation in this way makes things much quicker to compute. This is because modern numerical computing allows matrix multiplication to happen very fast. We don't need to serially calculate the beta-x products. We can do it all in parallel using vectorisation. However, we must always remember to have $x_{0}$ equal to 1. This is very important when we come to implement vectorised methods for neural networks, which makes them run much faster.\n",
    "\n",
    "Now our optimisation problem becomes easily stated. We must find the parameters $\\mathbf{β}$ for features $\\mathbf{x}$.\n",
    "$$ Parameters: \\mathbf{β} = \\{\\beta_{0}, ..., \\beta_{n}\\}$$\n",
    "$$ Features: \\mathbf{x} = \\{1, x_{1}, ..., x_{n}\\}$$\n",
    "Optimisation then becomes a problem of *parameter tuning*.\n",
    "\n",
    "### The Normal Equations\n",
    "\n",
    "With this in hand, now we can introduce the normal equations for finding the vector of parameters $\\mathbf{β}$. Provided with a dataset $\\{\\mathbf{X}, \\mathbf{y}\\}$, \n",
    "$$\\mathbf{β} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}$$\n",
    "\n",
    "This is a vectorised method, since it involves vectors and matrices of values to provide you with a vector of parameters. The maths behind why this works can be studied [here](https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/). \n",
    "1. You take the transpose of the X variable matrix, meaning that columns are now rows and rows are now columns, and multiply that by the X variable matrix. If $\\mathbf{X}$ is an m x n matrix, $\\mathbf{X}^{T}$ is an n x m matrix. The result of the product is a square n x n matrix, with dimensions equivalent to the number of predictor variables you have (n). You can't multiply in the opposite order, because matrix multiplication is not commutative.\n",
    "2. You then invert this matrix. Matrix inversion results in a matrix that when multiplied by the noninverted original matrix, results in the identity matrix. See [here](https://www.mathsisfun.com/algebra/matrix-inverse.html)\n",
    "3. You then multiply the result of step 2 by $\\mathbf{X}^{T}$ which is an n x m matrix. Multiplying an n x n with an n x m, results in an n x m matrix.\n",
    "4. You then multiply the result of step 3, an n x m matrix, by the m x 1 vector $\\mathbf{y}$, which results in an n x 1 vector $\\mathbf{β}$, with as many elements as predictors.\n",
    "\n",
    "If you haven't covered matrix multiplication, dot products, and matrix inversion, do not worry!\n",
    "\n",
    "Let's implement this and compare it to the output of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9567e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############## DO NOT CHANGE THIS CODE ########################\n",
    "\n",
    "iris_data_subset_x0 = iris_data_subset\n",
    "iris_data_subset_x0.loc[:,('x0')] = np.ones((100, 1))\n",
    "Xmatrix_df = iris_data_subset_x0[[\"x0\", \"sepallength\"]]\n",
    "Yvec_df = iris_data_subset_x0[[\"petallength\"]]\n",
    "X_matrix = Xmatrix_df.to_numpy()\n",
    "y_vec = Yvec_df.to_numpy()\n",
    "\n",
    "def normeq_fun(datax, datay):\n",
    "    beta_vec = np.dot(np.dot(np.linalg.inv(np.dot(datax.T,datax)),datax.T),datay)\n",
    "    return(beta_vec)\n",
    "\n",
    "normeq_fun(X_matrix, y_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850102e7",
   "metadata": {},
   "source": [
    "Depending on where you started your beta values in your gradient descent earlier, you may be close or quite far away from these values. This is because the gradients in the cost function are very shallow and so gradient descent is takes many iterations to converge. The closer you put your starting values to these outputs here, the quicker your algorithm will converge somewhere near these outputs. For linear regression, the normal equations are much faster than gradient descent. However, for many types of regression, there are no equations like the normal equations, and so gradient descent is our next best option.\n",
    "\n",
    "Now see how easy it is to implement regression with sci-kit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## DO NOT CHANGE THIS CODE ########################\n",
    "\n",
    "reg = LinearRegression().fit(X_matrix, y_vec)\n",
    "print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f50c5",
   "metadata": {},
   "source": [
    "This was a long tutorial! Now we have all the tools to start thinking about nonlinear extensions of linear regression. Next tutorial, we will look at logistic regression, one kind of *generalised linear model* that allows us to use linear regression techniques to predict non-linear phenomena. From there, we will be able to build a simple neural network, which is essentially lots of linked up linear regressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af431b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28edaff4",
   "metadata": {},
   "source": [
    "# An Introduction To Classification With Logistic Regression\n",
    "\n",
    "Author: K. Voudouris (2022)\n",
    "\n",
    "In this tutorial, we will extend linear regression to deal with binary variables, creating an algorithm that can classify examples into a binary set of categories, namely, logistic regression. Both linear and logistic regression form special cases of a broad family of models called Generalized Linear Models, used widely in statistics. They also form the basis for simple Artificial Neural Networks, which we will begin to investigate by looking at the perceptron and how that can be extended to multi-layer perceptron models.\n",
    "\n",
    "First, let's import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902ebdf",
   "metadata": {},
   "source": [
    "Next, we will import the dataset. This week, we will be using a dataset from [here](https://www.sheffield.ac.uk/mash/statistics/datasets), which includes various details about newborn babies. It includes the following variables:\n",
    "- ID\t                   \n",
    "Baby number\t\n",
    "- length\t               \n",
    "Length of baby (cm) \n",
    "- Birthweight\t           \n",
    "Weight of baby (kg)\t\n",
    "- headcirumference\t       \n",
    "Head Circumference\t\n",
    "- Gestation\t               \n",
    "Gestation (weeks)\t\n",
    "- smoker\t               \n",
    "Mother smokes 1 = smoker 0 = non-smoker\t\n",
    "- motherage\t               \n",
    "Maternal age\t\n",
    "- mnocig\t               \n",
    "Number of cigarettes smoked per day by mother\t\n",
    "- mheight\t               \n",
    "Mothers height (cm)\t \n",
    "- mppwt\t                   \n",
    "Mothers pre-pregnancy weight (kg)\t \n",
    "- fage\t                   \n",
    "Father's age\t \n",
    "- fedyrs\t               \n",
    "Father’s years in education\t\n",
    "- fnocig\t               \n",
    "Number of cigarettes smoked per day by father\t\n",
    "- fheight\t               \n",
    "Father's height (cm)\t \n",
    "- lowbwt\t               \n",
    "Low birth weight, 0 = No and 1 = yes\t \n",
    "- mage35\t               \n",
    "Mother over 35, 0 = No and 1 = yes\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5096e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "birthweight_data = pd.read_csv(\"./Birthweight_reduced_kg_R.csv\", na_values = ' ')\n",
    "\n",
    "birthweight_data.head() #show the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11545a",
   "metadata": {},
   "source": [
    "## Introducing Classification\n",
    "\n",
    "In linear regression up to now, we have been predicting values of *y* based on values of **x**. In the cases we looked at, the y-variable has been a continuous numerical scale, like the Sepal Width or Petal Length of an iris flower. But what happens if *y* only takes on a small number of discrete values. In this case, we don't want to output a value on a continuous scale, we want our prediction about *y* given **x** to be one of the discrete values. Essentially, we want to *classify* a new data point, characterised by a set of x-values, **x** as one of those discrete values. This clearly has important uses. Say we want to predict who whether a child has clinically low birth weight (a binary variable). It would make no sense to be predicting intermediate values between 0 and 1, because a baby can either be of low birth weight or it is not. Classification algorithms give us the tools to make predictions when the dependent variable is discrete, and intermediate values are meaningless. Many neural networks and other ML methods are classifiers, allowing us to make discrete predictions, rather than continuous ones.\n",
    "\n",
    "In this tutorial, we will focus on the case where the dependent variable, **y**, is binary. We will be looking at logistic regression to solve the binary classification problem. Logistic regression is one of the simplest and oldest classification techniques out there. Understanding it paves the way for a deeper understanding of how neural networks work and ultimately how to build better ones.\n",
    "\n",
    "First, let's see more clearly why linear regression is inadequate for classification problems such as these. We will make up a dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391415d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's make up a dataset first\n",
    "\n",
    "x =  np.linspace(-5, 5, 20)\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf222c",
   "metadata": {},
   "source": [
    "Let's implement linear regression to see what happens. We can implement it quickly using sklearn, as we saw last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(x.reshape(-1,1), y)\n",
    "pred = LR.predict(x.reshape(-1,1))\n",
    "plt.scatter(x, y, label=\"Actual\")\n",
    "plt.plot(x, pred, label=\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649aed4",
   "metadata": {},
   "source": [
    "Let's do the same with some real data from the Babyweight dataset. Let's theorise that the length of a newborn baby is predictive of whether that baby is of low birthweight. Fill in the appropriate bits of code. We want to plot `Length` on the x-axis and `lowbwt` on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64276c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Fill in the appropriate bits of code\n",
    "\n",
    "birthweight_data['Length'] = pd.to_numeric(birthweight_data['Length'], errors='coerce')\n",
    "birthweight_data['lowbwt'] = pd.to_numeric(birthweight_data['lowbwt'], errors='coerce')\n",
    "babylength = birthweight_data[''].values\n",
    "babyweight = birthweight_data[''].values\n",
    "\n",
    "\n",
    "LR.fit(babylength.reshape(-1,1), babyweight)\n",
    "pred = LR.predict(babylength.reshape(-1,1))\n",
    "plt.scatter(babylength, babyweight, label=\"Actual\")\n",
    "plt.plot(babylength, pred, label=\"Predicted\")\n",
    "plt.title('Babyweight')\n",
    "plt.xlabel('Length of Baby at birth \\cm')\n",
    "plt.ylabel('Low Birth Weight Binary (1 = low birthweight)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ddecc",
   "metadata": {},
   "source": [
    "Clearly, using linear regression as we have done up to now is not working well for this classification problem. Indeed, in general linear regression suffers from 3 problems here:\n",
    "1. A high error rate - a lot of the predicted y-values, $\\hat{y}$, are very different from the real y-values, **y** in the dataset, especially as *x* tends towards $\\infty$ and $-\\infty$.\n",
    "2. The model predicts values greater than 1 and less than 0 a lot of the time.\n",
    "3. It is very sensitive to outliers, see below for what would happen to our predictor if a baby was born 4ft in length (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc41ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "babylength_outlier = np.append(babylength, 120)\n",
    "babyweight_outlier = np.append(babyweight, 0)\n",
    "\n",
    "LR.fit(babylength_outlier.reshape(-1,1), babyweight_outlier)\n",
    "pred = LR.predict(babylength_outlier.reshape(-1,1))\n",
    "plt.scatter(babylength_outlier, babyweight_outlier, label=\"Actual\")\n",
    "plt.plot(babylength_outlier, pred, label=\"Predicted\")\n",
    "plt.title('Babyweight with Outlier')\n",
    "plt.xlabel('Length of Baby at birth \\cm')\n",
    "plt.ylabel('Low Birth Weight Binary (1 = low birthweight)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74691dc7",
   "metadata": {},
   "source": [
    "## Introducing Logistic Regression\n",
    "\n",
    "So, the linear regression model representation below is insufficient for these classification algorithms. As a reminder, the linear regression model representation is this:\n",
    "$$\\hat{y} = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "It can also be written as a *hypothesis function*, basically stating what our hypothesis is about the dependent variable given a bunch of x-values.\n",
    "$$h_{\\beta}(\\mathbf{x}) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{n}x_{n}$$\n",
    "In vectorised notation, this can be written as:\n",
    "$$h_{\\beta}(\\mathbf{x}) = \\mathbf{β}^{T}\\mathbf{x}$$ \n",
    "*where $x_{0} = 1$*\n",
    "\n",
    "We will proceed with the latter notation for simplicity, but note that all three of the equations above are just notational variants of each other. Revisit the last section of the previous tutorial if you wish to understand why.\n",
    "\n",
    "What we would like to do is adapt the hypothesis function above so that our output is bounded between 0 and 1. We do this by using a *link function*, or (in ML) an *activation function*, which essentially transforms an output in a certain way to give us a some desired properties. There are several functions that can transform the output of the linear function above from y-values below 0 and above 1 to y-values bounded between 0 and 1. Here, we will use the **logistic function**, **logit**, or **sigmoid function**, which has some handy properties which is why it is so ubiquitous.\n",
    "\n",
    "The logistic function looks like this:\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "What this means is that whatever value you take in as *z*, you transform it by putting negative z (-z) as the exponent of *e* (Euler's number), adding 1 to that, and then dividing 1 by the result. You don't need to fully understand what this means, the important thing is that g(z) is always greater than 0 and less than 1. More formally:\n",
    "$$0<g(z)<1$$\n",
    "\n",
    "We can now wrap this function around our linear regression model representation from earlier, and only generate results between 0 and 1:\n",
    "$$h_{\\beta}(\\mathbf{x}) = g(\\mathbf{β}^{T}\\mathbf{x}) = \\frac{1}{1+e^{-\\mathbf{β}^{T}\\mathbf{x}}}$$\n",
    "\n",
    "Before we get into the details, it's worth pointing out that we can now simply state what we need to do to solve these kinds of classification problems. We need to find values of $\\mathbf{β}$ that result in small error between $h_{\\beta}(\\mathbf{x})$ (or $\\hat{y}$) as produced by the above model representation, and actual values of *y* in our dataset.\n",
    "\n",
    "Now write a function that computes the logistic function given a vector of values, note that the exponent function is `np.exp()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a function to calculate values of g(z) given a vector of z values\n",
    "\n",
    "def sigmoid_fun(vector):\n",
    "    ##Insert code here\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee349b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sigmoid =  np.linspace(-5, 5, 1000)\n",
    "y_sigmoid = sigmoid_fun(x_sigmoid)\n",
    "plt.scatter(x_sigmoid, y_sigmoid)\n",
    "plt.title('Sigmoid Curve')\n",
    "plt.xlabel('Linear model, beta-transpose-x')\n",
    "plt.ylabel('Hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e3448",
   "metadata": {},
   "source": [
    "What are the axes here? The x-axis is $\\beta^{T}x$, the set of x values and their beta coefficients, just as in the linear model. In the simplest case, where $\\beta_{0} = 0$ (the y-intercept) and $\\beta_{1} = 1$ (the gradient associated with x), the x-axis would just be the raw x-values. The y axis is the result of transforming $\\beta^{T}x$ according to the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8839fc",
   "metadata": {},
   "source": [
    "### A Note on Generalised Linear Models\n",
    "\n",
    "As mentioned, there are several link functions that can be used that increase smoothly between two values. The *tanh* (pronounced *tansh* or *tanch*) function is commonly used in ML. The log link function is used often in statistics for Poisson regression. These all constitute members, along with plain old linear regression, of the **Generalised Linear Model** family (GLM). They are all the same in nature, with just the details of the link function differing.\n",
    "\n",
    "There are two main ways to interpret GLMs. The first is to interpret them geometrically, in terms of minimising the distance between what the model predicts and the actual values in the dataset. The second is to interpret them probabilistically, in terms of maximising the probability that the predicted value matches the actual value in the dataset. This is the maximum likelihood approach, which I will introduce here. Maximum likelhood estimation can be applied to the linear regression cases we have already discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce38916",
   "metadata": {},
   "source": [
    "## The Probabilistic (Maximum Likelihood) Interpretation of Logistic Regression\n",
    "\n",
    "(NOTE: this section borrows a lot from [Andrew Ng's Stanford Lecture Notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf), credit where credit is due)\n",
    "\n",
    "There is still a problem with the logistic function we have got. While we have bounded our hypothesis function to be between 0 and 1, it still predicts values of *y* that are not real in some sense. What we can do is assume that the sigmoid curve instead provides a probability that some set of x-values is truly categorised as 0 or 1. Assuming this brings in some assumptions about the data, which we will discuss below. For now, let's show how we can interpret the output of the sigmoid function as a probability.\n",
    "\n",
    "Let's assume that:\n",
    "$$P(y=1|x;β) = h_{β}(x)$$\n",
    "$$P(y=0|x;β) = 1 - h_{β}(x)$$\n",
    "\n",
    "This means that the probability of y being equal to 1, **given a some x-values and some beta-values**, is equal to the output of our model representation, i.e., the point on the sigmoid curve corresponding to our x-values.\n",
    "By definition, the probability of y being equal to 0 is one minus the probability of it being equal to 1.\n",
    "\n",
    "We can re-write these two equation as one:\n",
    "$$p(y|x;β) = (h_{β}(x))^{y}(1 - h_{β}(x))^{1-y}$$\n",
    "\n",
    "To see how this is equivalent to the above two equations, replace all ys with 1s and then replace all ys with 0s.\n",
    "\n",
    "\n",
    "\n",
    "For y = 1:\n",
    "$$p(y=1|x;β) = (h_{β}(x))^{y = 1}(1 - h_{β}(x))^{1-(y=1)}$$\n",
    "Which is equivalent to:\n",
    "$$p(1|x;β) = (h_{β}(x))^{1}(1 - h_{β}(x))^{1-1}$$\n",
    "Which is equivalent to (recall that something to the power of 0 is 1):\n",
    "$$p(1|x;β) = (h_{β}(x))^{1}(1 - h_{β}(x))^{0}$$\n",
    "Which is equivalent to:\n",
    "$$p(1|x;β) = (h_{β}(x))^{1}*1$$\n",
    "Which is equivalent to:\n",
    "$$p(1|x;β) = (h_{β}(x))$$\n",
    "\n",
    "\n",
    "\n",
    "For y = 0:\n",
    "$$p(y=0|x;β) = (h_{β}(x))^{y = 0}(1 - h_{β}(x))^{1-(y=0)}$$\n",
    "Which is equivalent to:\n",
    "$$p(0|x;β) = (h_{β}(x))^{0}(1 - h_{β}(x))^{1-0}$$\n",
    "Which is equivalent to:\n",
    "$$p(0|x;β) = (h_{β}(x))^{0}(1 - h_{β}(x))^{1}$$\n",
    "Which is equivalent to:\n",
    "$$p(0|x;β) = 1*(1 - h_{β}(x))^{1}$$\n",
    "Which is equivalent to:\n",
    "$$p(0|x;β) = (1 - h_{β}(x))$$\n",
    "\n",
    "Now we can interpret the output of the sigmoid function as a probability, this gives us a really useful tool for defining a cost function to optimise logistic regression.\n",
    "\n",
    "## Cost Functions for Logistic Regression\n",
    "\n",
    "How do we go about finding the coefficients (or parameters) **β** that maximise the probability that y = 0 or y = 1, given a set of x-values? In other words, which beta values maximise the likelihood that y = 0 or y = 1? Just as we did with Mean Squared Error in the previous tutorial, we want to find the optimum beta values, and to do this we use a cost function. However, in this case, we don't want to minimise the function, we want to *maximise* it. Other than that, the problem is exactly the same.\n",
    "\n",
    "To do this, we need to know how the probability of y given x changes depending on different beta values. We can write this down as a *likelihood function*:\n",
    "$$L(β) = p(y|x;β)$$\n",
    "This computes how the probability changes as we adjust beta. \n",
    "\n",
    "Let's imagine a very simple dataset with one predictor variable, x, and one predicted variable, y. Now, in our dataset, we have a list of 0s and 1s as our y-values and a list of x-values of whatever variety (but converted to numerical values). Given a candidate beta value, we want to calculate the probability that y is a 0 or a 1, and we need to do this for each of x-values in the list. So, for a candidate x value, *i*, we compute the probability using the above equations as follows $p(y^{(i)}|x^{(i)};β)$. We want to iterate through all the x-values and get a probability that y equals whatever it equals for each x. When we have all these probabilities, we want to combine them to give a likelihood value for that particular beta value. We can't simply add them all together, like we did with Mean Squared Error, because this would often result in probabilities over 1. Instead, we multiply them all together (creating the joint probability), which we denote with a large π symbol. For m training examples, we get:\n",
    "$$L(β) = \\prod^{m}_{i = 1}p(y^{(i)}|x^{(i)};β)$$\n",
    "Breaking that down into the likelihood function we have above, we get:\n",
    "$$L(β) = \\prod^{m}_{i = 1}(h_{β}(x^{(i)}))^{y^{(i)}}(1 - h_{β}(x^{(i)}))^{1-y^{(i)}}$$\n",
    "\n",
    "The likelihood function gives us the likelihood of a y-value, $y^{(i)}$, in our dataset based on our model, $h_{β}(x)$ and an x-value $x^{(i)}$. When our model is good at predicting y-values, the likelihood value should get closer to 1. Let's see how this works by looking at some concrete cases.\n",
    "\n",
    "Good case:\n",
    "$$h_{β}(x) = 0.75, y = 1$$\n",
    "$$L(β) = 0.75^{1}*(1-0.75)^{(1-1)} = 0.75$$\n",
    "$$h_{β}(x) = 0.15, y = 0$$\n",
    "$$L(β) = 0.15^{0}*(1-0.15)^{(1-0)} = 0.85$$\n",
    "$$Joint Probability: 0.75*0.85 = 0.6375$$\n",
    "\n",
    "Bad case:\n",
    "$$h_{β}(x) = 0.25, y = 1$$\n",
    "$$L(β) = 0.25^{1}*(1-0.25)^{(1-1)} = 0.25$$\n",
    "$$h_{β}(x) = 0.85, y = 0$$\n",
    "$$L(β) = 0.85^{0}*(1-0.85)^{(1-0)} = 0.15$$\n",
    "$$Joint Probability: 0.25*0.15 = 0.0375$$\n",
    "\n",
    "So, we want to find the beta parameter(s) that maximise the likelihood.\n",
    "\n",
    "There is one small adjustment left to make. Just as we can maximise the likelihood, $L(β)$, we can also maximise any function of $L(β)$ (so long as it strictly increases). To make things easier, we therefore transform the likelihood function into the *log likelihood*, taking the natural logarithm of the likelihood:\n",
    "$$l(β) = log(L(β))$$\n",
    "Which can be expanded to the following:\n",
    "$$l(β) = \\sum^{m}_{i = 1}[y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)}))]$$\n",
    "\n",
    "The reason we do this is that summation is computationally more efficient than the product, so it makes it quicker to run. Otherwise, the intuition is exactly the same. Now we just have to maximise $l(β)$. Note that the log likelihood, unlike the likelihood, can be greater than 1 and less than 0, because it is the natural logarithm of the probability.\n",
    "\n",
    "That's a *lot* of equations! Let's try it out with some data to try to get the intuition. Let's take it step-by-step. First, let's plot whether or not a baby was of low birth weight as a function of length again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthweight_data['Length'] = pd.to_numeric(birthweight_data['Length'], errors='coerce')\n",
    "birthweight_data['lowbwt'] = pd.to_numeric(birthweight_data['lowbwt'], errors='coerce')\n",
    "babylength = birthweight_data['Length'].values\n",
    "babyweight = birthweight_data['lowbwt'].values\n",
    "\n",
    "plt.scatter(babylength, babyweight)\n",
    "\n",
    "plt.title('Babyweight')\n",
    "plt.xlabel('Length of Baby at birth \\cm')\n",
    "plt.ylabel('Low Birth Weight Binary (1 = low birthweight)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50801a5f",
   "metadata": {},
   "source": [
    "Now we will build the logistic regression. Recall that our equation is this:\n",
    "$$h_{\\beta}(\\mathbf{x}) = g(\\mathbf{β}^{T}\\mathbf{x}) = \\frac{1}{1+e^{-\\mathbf{β}^{T}\\mathbf{x}}}$$\n",
    "\n",
    "First, build a function that calculates $\\mathbf{β}^{T}\\mathbf{x}$, just as we did with linear regression. Recall that this is identical to $\\mathbf{y} = \\beta_{0} + \\beta_{1}\\mathbf{x}_{1}$. \n",
    "\n",
    "It needs to take as input a y-intercept, $β_{0}$, a gradient coefficient $β_{1}$, and a vector of x's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e407ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linregress(beta0, beta1, datax):\n",
    "    ##Insert code here\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294573cb",
   "metadata": {},
   "source": [
    "Now use the output of this function, a vector, as input to your sigmoid function from earlier. Play around with the beta parameters to find ones that work. This is essentially squidging the linear regression between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced36a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b0 = 49\n",
    "b1 = -1.03\n",
    "\n",
    "z = linregress(b0, b1, babylength)\n",
    "LogitTransformedVector = sigmoid_fun(z)\n",
    "\n",
    "plt.scatter(babylength, babyweight)\n",
    "plt.scatter(babylength, LogitTransformedVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584edfe",
   "metadata": {},
   "source": [
    "Using these functions that you have built, build a function that calculates the log likelihood:\n",
    "$$l(β) = \\sum^{m}_{i = 1}[y^{(i)}log(h(x^{(i)}))+(1-y^{(i)})log(1-h(x^{(i)}))]$$\n",
    "\n",
    "It should take as input beta0, beta1, a vector of x values and a vector of y values.\n",
    "\n",
    "The function for the natural logarithm is `np.log()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483148b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_fun(beta0, beta1, datax, datay):\n",
    "    ## Insert code here\n",
    "    \n",
    "    return(loglikelihood)\n",
    "\n",
    "loglikelihood_fun(50, -0.5, babylength, babyweight) # you should get a value of -863.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834acb05",
   "metadata": {},
   "source": [
    "Let's plot a range of values of the log likelihood for different values of beta0 and beta1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2755d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0, beta1 = np.meshgrid(np.linspace(48,52,100), np.linspace(-1.2,-0.8,100)) # create a bunch of values that define a grid surface\n",
    "beta0 = np.ndarray.flatten(beta0) #flatten them to put in a dataframe\n",
    "beta1 = np.ndarray.flatten(beta1)\n",
    "\n",
    "ll_df = {\n",
    "    'beta0': beta0,\n",
    "    'beta1': beta1} # make a dictionary\n",
    "ll_df = pd.DataFrame(ll_df) #convert it to a dataframe\n",
    "\n",
    "ll_df['LogLikelihood'] = ll_df.apply(lambda row : loglikelihood_fun(row['beta0'], row['beta1'], babylength, babyweight), axis = 1) # this function applies your function to every beta0 and beta1 value\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('beta0', fontsize = 15) #change the axis labels as necessary\n",
    "ax.set_ylabel('beta1', fontsize = 15)\n",
    "ax.set_zlabel('Log Likelihood', fontsize = 15)\n",
    "ax.scatter(ll_df['beta0'], ll_df['beta1'], ll_df['LogLikelihood'], alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7b443",
   "metadata": {},
   "source": [
    "As with linear regression, there are a range of values for β that are optimal. The job of the machine learning algorithm is to find the optimum. In this case, we want to maximise likelihood, so we use **gradient ascent**, which is exactly like the **gradient descent**, except we ascend *up* the gradient rather than down. The log likelihood curve has the neat property of being convex, so the local minimum is always the global minimum. However, there is currently no closed form solution like the Normal Equations to find this minimum, so we have to use an iterative algorithm like **gradient ascent**.\n",
    "\n",
    "Gradient ascent has an update rule like this:\n",
    "$$\\beta_{0} := \\beta_{0} + \\alpha\\frac{\\partial}{\\partial (\\beta_{0})}l(\\beta_{0}, \\beta_{1})$$\n",
    "$$\\beta_{1} := \\beta_{1} + \\alpha\\frac{\\partial}{\\partial (\\beta_{1})}l(\\beta_{0}, \\beta_{1})$$\n",
    "\n",
    "Note the positive rather than negative sign in the update, since we are maximising rather than minimising the cost function.\n",
    "\n",
    "The partial derivative of the log likelihood is nice and simple, because of the nature of the logistic function (which is why we choose it!). You can check out the derivation [here on page 18](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf).\n",
    "Suffice it to say, our update rule looks like this:\n",
    "$$\\beta_{0} := \\beta_{0} + \\alpha(y^{(i)} - h_{β}(x_{0}^{(i)}))$$\n",
    "$$\\beta_{1} := \\beta_{1} + \\alpha(y^{(i)} - h_{β}(x_{1}^{(i)}))x_{1}^{i}$$\n",
    "This looks very similar to our gradient descent algorithm for linear regression. However, it is different in that $h_{β}(x^{(i)})$ is the result of a logistic function, not our linear model representation from the previous tutorial.\n",
    "\n",
    "### Interpreting the coefficients\n",
    "\n",
    "You may know that interpreting logistic regression is usually done in terms of odds ratios. This comes from rearranging the logistic function so that we can interpret what the coefficients **β** mean. If the probability, p, is defined as $\\frac{1}{1+e^{-\\mathbf{β}^{T}\\mathbf{x}}}$, then we can understand $\\mathbf{β}^{T}\\mathbf{x}$ as $log(p/(1-p))$. This is the log of the ratio between the probability of y = 1 and the probability of y = 0. We can then understand the coefficients **β** just as we understood them in linear regression. A one unit increase in *x* results in a *β* unit increase in the log odds of being categorised as y = 1. \n",
    "\n",
    "The probabilistic interpretation of logistic regression is extremely useful, allowing us to quantify the probability that some new data are a member of a particular category, rather than giving a binary yes/no answer to whether they are, or generating meaningless intermediary values. However, it does import some assumptions. The most important of these is that the data are independent and drawn from the same statistical distribution (the normal distribution), which is why we have to check for normality violations when using logistic regression in statistics.\n",
    "\n",
    "As has been clear over these tutorials, these methods are general to as many independent variables as you want. There are also extensions of logistic regression for multiple dependent variables and for discretely ordered dependent variables with more than 2 categories.\n",
    "\n",
    "Try implementing logistic regression with the scikit-learn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "reg = model.fit(babylength.reshape(-1,1), babyweight)\n",
    "print(reg.coef_, reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec4fcd",
   "metadata": {},
   "source": [
    "Using the above code as a template, try fitting a logistic regression model to predict babyweight using several independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa76cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46cf0fac",
   "metadata": {},
   "source": [
    "## A Geometric Intuition about Classifiers\n",
    "\n",
    "Classifiers can also be interpreted geometrically, although it is somewhat less rigorous (in my opinion). We can think of the logistic regression as finding a *linear separator*, a line or (hyper)plane that divides up the cluster of y=1 and y=0. Let's plot low birthweight as a function of length and head circumference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfc517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "birthweight_data['Length'] = pd.to_numeric(birthweight_data['Length'], errors='coerce')\n",
    "birthweight_data['lowbwt'] = pd.to_numeric(birthweight_data['lowbwt'], errors='coerce')\n",
    "birthweight_data['Headcirc'] = pd.to_numeric(birthweight_data['Headcirc'], errors='coerce')\n",
    "\n",
    "plt.scatter(birthweight_data.Length, birthweight_data.Headcirc, c=birthweight_data.lowbwt, s=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077646d",
   "metadata": {},
   "source": [
    "What logistic regression does is tries to find a line that separates out the yellow dots from the purple dots. Play around with the slope and intercept parameters to try to plot a line that separates the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOPE = -1.5\n",
    "INTERCEPT = 110\n",
    "x_vals = np.linspace(40,60,1000)\n",
    "y_vals = INTERCEPT + (SLOPE * x_vals)\n",
    "\n",
    "plt.scatter(birthweight_data.Length, birthweight_data.Headcirc, c=birthweight_data.lowbwt, s=75)\n",
    "plt.plot(x_vals, y_vals, '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d16c5",
   "metadata": {},
   "source": [
    "We can think of the logistic regression as finding the optimum line separating the groups, which is why the classic linear regression equation turns up in the logistic function from earlier. We can think of the maximum likelihood estimation as minimising the number of 'yellow' points on the 'purple' side of the line, and the number of 'purple' points on the 'yellow' side of the line, and the distance of those *misclassified* points from the linear separator. You can formalise this using tools from linear algebra, namely vector norms, but we won't do that here.\n",
    "\n",
    "Using the distance of misclassified points from the linear separator makes where we place the line very sensitive to outliers. If there was a yellow point in the top right of the above plot, it would shift the optimum line up a bit, to minimise the overall distance between the line and the misclassified points. To get around this, we need a function that diminishes the effect of these outliers as they get further and further away from the main cluster of points. The logistic function does exactly this, as it tapers off towards 0 and 1. So now we have another motivation for the logistic function for the geometric interpretation. It makes finding the linear separator far less sensitive to outliers than just using a plain old linear regression line-of-best-fit.\n",
    "\n",
    "It's a bit complicated to understand how the geometric interpretation and the probabilistic interpretation of logistic regression line up. I find it easier to think of them as complementary perspectives on the classification problem, and they ultimately lead to the same result via different means. However, they do highlight different useful properties of linear regression:\n",
    "1. The probabilistic interpretation allows us to quantify the probability that some data are a member of a particular category. However, the data need to be independently sampled and identically distributed according to the Normal distribution for us to do this.\n",
    "2. The geometric interpretation highlights that logistic regression is finding a *linear separator* between the two groups (in binary logistic regression), and gives us an intuition as to why we can wrap the classic linear regression model representation of y = mx + c inside the logistic function. However, as we will see below, this means that logistic regression can only be used with *linearly separable* data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b05cab",
   "metadata": {},
   "source": [
    "## The Importance of Logistic Regression for Machine Learning\n",
    "\n",
    "You have probably come across logistic regression in statistics classes. However, it is very important to machine learning, since it is very similar to the *perceptron learning algorithm*. The perceptron was developed by Frank Rosenblatt in 1958 and further developed during the 60s. It was thought to be an approximate model of how neurons work when it comes to classifying things, by working in an all-or-nothing manner. It was used to classify rudimentary images in the late 50s, by outputting whether an image was a member of one of two classes as a yes/no answer.\n",
    "\n",
    "It did this by having a step-like function, instead of our logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce41057",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "x = np.append(x, np.linspace(0,0,50))\n",
    "y = np.linspace(0, 0, 50)\n",
    "y = np.append(y, np.linspace(1,1,50))\n",
    "y = np.append(y, np.linspace(0,1,50))\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4dd80",
   "metadata": {},
   "source": [
    "The function can be written as:\n",
    "$$\\begin{equation}\n",
    "    s(z)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ z < 0 \\\\\n",
    "      1, & \\text{if}\\ z > 0 \\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "$$\n",
    "\n",
    "The step function outputs a value of 0 if z is less than 0 and a 1 if z is more than 0. The threshold for the step can be changed depending on the application. If we let z be $β^{T}x$, our linear separator, then we are forcing the algorithm to classify things on either side of the linear separator. The perceptron algorithm can then learn what the best values of beta are using gradient ascent, using the following update:\n",
    "$$\\beta_{0} := \\beta_{0} + \\alpha(y^{(i)} - s(β_{0}x_{0}^{(i)}))$$\n",
    "$$\\beta_{1} := \\beta_{1} + \\alpha(y^{(i)} - s(β_{1}x_{1}^{(i)}))x_{1}^{i}$$\n",
    "\n",
    "Whilst the algorithm is a bad representationof the data, $(y - s(βx))$ will be either -1 or 1, meaning the gradient ascent will be taking big steps. As it gets better, these values will become 0, and the gradient ascent rule will converge on optimal beta values.\n",
    "\n",
    "The perceptron algorithm is therefore identical to logistic regression, except that instead of the logistic function, it has this step-like function. However, that makes it difficult to interpret the output of the perceptron probabilistically or use maximum likelihood estimation to optimise it.\n",
    "\n",
    "The perceptron is important in the history of machine learning, because it forms the basis of simple feedforward neural networks. However, we usually don't use the step-function from the original perceptron, and instead use the logistic function because of its utility.\n",
    "\n",
    "### The First AI Winter\n",
    "\n",
    "The perceptron algorithm was a step forward in machine learning, allowing rudimentary classification with only little human input. Everyone was very excited. But then Marvin Minsky and Seymour Papert published a book demonstrating some of the limits of the perceptron algorithm, and by extension, logistic regression. This led to an abrupt halt to AI research until connectionism in the 80s. Let's look at one case of the limitations of logistic regression.\n",
    "\n",
    "The geometric interpretation of logistic regression led us to the conclusion that it finds a linear separator between groupings. This is problematic, because not all data are linearly separable. Take the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.linspace(0,5,20), np.linspace(0,5,20))\n",
    "x = np.ndarray.flatten(x)\n",
    "y = np.ndarray.flatten(y)\n",
    "labs = np.linspace(0,0,400)\n",
    "\n",
    "for i in range(0,399):\n",
    "    if x[i] > 1 and x[i] < 4 and y[i] > 1 and y[i] < 4:\n",
    "        labs[i] = 1\n",
    "\n",
    "plt.scatter(x, y, c=labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12c9ca",
   "metadata": {},
   "source": [
    "This is known as the 'donut problem'. Clearly, a linear separator is inadequate here.\n",
    "\n",
    "The achilles heel for the perceptron back in the day was another problem. Researchers back then were interested in modelling logic using computers. It was spotted that the perceptron and logistic regression are unable to learn an XOR rule. The XOR rule goes like this:\n",
    "\n",
    "| X1 | X2 | X1 XOR X2 |\n",
    "| --- | --- | --- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "The XOR operator outputs 1 only when the two elements are exclusive, i.e., the have different values, and outputs 0 when the two elements are not exclusive, i.e. they have the same values. If you look at the below plot, you'll see that you can't draw a line separating the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c616995",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0,0,1,1])\n",
    "x2 = np.array([0,1,0,1])\n",
    "XOR = np.array([0,1,1,0])\n",
    "\n",
    "plt.scatter(x1, x2, c=XOR, s = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30bfcd9",
   "metadata": {},
   "source": [
    "Logistic regression, as a linear classifier, cannot classify these when we just take X1 and X2 as input. There are ways to classify the output of the XOR rule. The first is to use a new input, the product of X1 and X2:\n",
    "\n",
    "| X1 | X2 | X1\\*X2 | X1 XOR X2 |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58846cc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x1, x2, x1*x2, c=XOR, s=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e4da9",
   "metadata": {},
   "source": [
    "There is a plane that can divide the two groups, now we take the product of the two variables. See if you can use scikit-learn to build a logistic regression that can correctly classify all these points, and compare it to the logistic regression that uses only X1 and X2 as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "620ac339",
   "metadata": {},
   "source": [
    "This works nicely for XOR, but it doesn't always solve the problem when data are not linearly separable. For example, it struggles to find a linear separator for the donut problem above. \n",
    "\n",
    "What we need is a something that can find non-linear separators. That is what neural networks do. We can link up several logistic regressions, which take as input the output of another logistic regression, enabling us to find 'wiggly' separators when the thing we are trying to classify is complex. You can think of a neural network as a set of linked up logistic regressions, which is why they are sometimes called 'Multi-layer Perceptrons'. This is what connectionists like Geoffrey Hinton, James McClelland, and David E. Rumelhart innovated in the 1980s, ending the first AI Winter.\n",
    "\n",
    "We will leave neural networks to the next tutorial. However, we now have all the tools to really understand what they are doing. You can think of Neural Networks as just more complex regressions (logistic and linear). They use link functions (like the logistic), cost functions, and gradient descent/ascent, and just apply them to contexts too complex for linear methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea1e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

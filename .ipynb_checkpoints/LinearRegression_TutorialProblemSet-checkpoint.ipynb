{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e425c02",
   "metadata": {},
   "source": [
    "# $\\mathbf{\\text{Linear Regression: A Tutorial}}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a1627",
   "metadata": {},
   "source": [
    "Author: K. Voudouris, 2021\n",
    "\n",
    "Please note: This tutorial has borrowed heavily from [here](https://mubaris.com/posts/linear-regression/#:~:text=Linear%20Regression%20from%20Scratch%20in%20Python%201%20Simple,me%20know%20if%20you%20found%20any%20errors.%20).\n",
    "\n",
    "First, we import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073fc196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bc980",
   "metadata": {},
   "source": [
    "Next we import the Iris dataset from sklearn. The Iris dataset is the classic dataset for Data Science.It contains five columns about Iris flowers: Petal Length, Petal Width, Sepal Length, Sepal Width, and Species Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv(\"./iris_csv.csv\")\n",
    "iris_data.head() #show the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.sample(10) #show a random sample of 10 of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4830ca",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Explore the dataset using the pandas functions:\n",
    "\n",
    "columns()\n",
    "\n",
    "shape()\n",
    "\n",
    "iloc()          Identify columns by their row index\n",
    "\n",
    "loc()           Identify columns by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "##Write Code here:\n",
    "\n",
    "\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514cda4",
   "metadata": {},
   "source": [
    "## $\\mathbf{\\text{Introducing Linear Regression}}$<br>\n",
    "\n",
    "We will be focusing here on simple bivariate linear regression. This means we try to model how a change in one variable (the independent/explanatory/predictor variable) is related to a change in other variable (dependent/predicted variable). Usually, the independent variable is denoted by $\\mathbf{X}$ and the dependent by $\\mathbf{y}$.\n",
    "\n",
    "Linear regression, in its simplest bivariate form, can model a linear relationship between a vector of values $\\mathbf{x}$ and a vector of values $\\mathbf{y}$. It essentially fits a line of best fit. This line has the familiar mathematical characterisation from high-school maths classes of $\\mathbf{y = mx + c}$. $\\mathbf{m}$ is the (gradient/slope/scale) coefficient, it tells you how much a value *y* changes when you change a value of *x* by 1 unit. $\\mathbf{c}$ is the intercept (or sometimes the bias coefficient). It tells you what we expect *y* to be when *x* is 0 (and therefore $\\mathbf{mx}$ is 0).\n",
    "\n",
    "Usually, we rearrange this equation and use different letters when we talk in DataScience/ML contexts. This is because the new notation generalises better when we think about multivariate cases and more general modelling techniques. However, they are essentially the same. The following is used to characterise the bivariate linear model:\n",
    "\n",
    "$$\\mathbf{y} = \\beta_{0} + \\beta_{1}\\mathbf{x}$$\n",
    "\n",
    "Our job is to find the best values of $\\beta_{0}$ and $\\beta_{1}$ for our dataset containing {$\\mathbf{x, y}$}\n",
    "\n",
    "Let's plot some data with Iris. Let's assume that we are interested in how petal width changes based on petal length.\n",
    "\n",
    "Find the columns of interest and assign them to x and y variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############Fill in the parts of the code necessary\n",
    "\n",
    "x = iris_data[''].values\n",
    "y = iris_data[''].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec6986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########Plot the data using matplotlib\n",
    "\n",
    "plt.scatter(x, y, label = 'Scatter Plot of Iris Dataset')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018e799",
   "metadata": {},
   "source": [
    "Now let's think about lines of best fit. Play around with the following code to fit lines of best fit to your graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######We need a set of x-values first, between 0 and 7\n",
    "\n",
    "min_x = 0\n",
    "max_x = 7\n",
    "\n",
    "xvalues = np.linspace(min_x, max_x, 500) #creates 500 values between 0 and 7\n",
    "\n",
    "b0 = 2 #vary the intercept\n",
    "b1 = 0 #vary the gradient coefficient\n",
    "\n",
    "yvalues = b0 + b1 * xvalues #create a vector of yvalues\n",
    "\n",
    "plt.plot(xvalues, yvalues, label = 'Line of Best Fit')\n",
    "plt.scatter(x, y, label = 'Data points')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d533a",
   "metadata": {},
   "source": [
    "We could do this by eye and get an okay looking plot, as I am sure you have. But ideally we would at least have a metric to determine how well this line fits (i.e., a measure of model-fit). One such metric is the *Root Mean Squared Error (RMSE)*. \n",
    "\n",
    "Our line of best fit (our *model*) gives an expected *y* value, $\\hat{y}$, for a given value of *x*. For the x-values we know about, how different is $\\hat{y}$ from *y*? We can simply calculate that difference for each of our datapoints. To give an overall metric for the *error* of our model, we sum all those values. The problem is that some of these values will be negative and some will be positive, and we don't want those to cancel each other out (i.e., say we have 0 error, when actually we have -2+2 error). So the solution is to square all these errors to give just positive values. Now we want the average error for your average data point, so we can divide the sum of all the squared errors by the number of squared errors (i.e. the number of x-values we have) to give an average error. The final issue is that this average error is in units of $y^2$. So Let's square root the whole thing to give us a standardised metric of error across our dataset in units of *y*. The equation is as follows:\n",
    "\n",
    "$$RMSE = \\sqrt{\\sum\\limits _{j = 1} ^{m}\\frac{1}{m}(\\hat{y}_{j}-y_{j})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = 2 #vary the intercept\n",
    "b1 = 0 #vary the gradient coefficient\n",
    "\n",
    "y_expected = b0 + b1 * x #create a vector of expected values\n",
    "\n",
    "#####################Calculate the RMSE for your chosen values of b0 and b1\n",
    "##Complete code here\n",
    "\n",
    "differences = y_expected - y\n",
    "sq_differences = differences*differences\n",
    "length_m = len(y)\n",
    "#...\n",
    "#note sqrt is math.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################BONUS: make this into a function#######################\n",
    "\n",
    "def RMSE_calc(b0, b1):\n",
    "    ## enter code here\n",
    "    return(rmse)\n",
    "\n",
    "print(RMSE_calc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2f479",
   "metadata": {},
   "source": [
    "There is another metric for determining fit here, $R^{2}$. RMSE gave us the average error in units of *y*. $R^{2}$ gives us a measure of the proportion of the total variance (strictly speaking, a value *proportional* to the total variance) in the data that is accounted for by the variance in the error between the data and the model you have fitted. $R^{2}$ uses the Sum-of-Squares, and is the Sum-of-Squares of the residuals (the errors) of the model divided by the total Sum-of-Squares of the data relative to the mean.\n",
    "\n",
    "The following is the total sum of the squares of the *residuals*, i.e. the differences between the actual *y* values in the dataset and the predicted *y* values from our model:\n",
    "\n",
    "$$SS_r = \\sum\\limits _{j = 1} ^{m}(y_{j}-\\hat{y}_{j})^2$$\n",
    "\n",
    "Then we have the total sum of squares of the data, which is the total sum of the squares of the differences between actual *y* values and the mean of *y*, $\\bar{y}$. This is kind of like the variance of the data, although that is usually scaled by $\\frac{1}{N(-1)}$. $SS_t$ is the total of how much the datapoints vary around the mean of *y*.\n",
    "\n",
    "$$SS_t = \\sum\\limits _{j = 1} ^{m}(y_{j}-\\bar{y}_{j})^2$$\n",
    "\n",
    "We want to be able to come with an explanation for $SS_t$, and introducing a line of best fit helps to do that be introducing the effect of a predictor variable, *x*. We want the error around our fitted model to be small, relative to the amount of variability inherent in the data set. To calculate this relative effect, we simply divide $SS_r$ by $SS_t$.\n",
    "\n",
    "$$\\frac{SS_r}{SS_t}$$\n",
    "\n",
    "For a given value, this gives a smaller value the smaller the error around the fitted model, which is what we want. To put this onto a scale where larger values mean better fit, we subtract this fraction from one, to give us $R^{2}$.\n",
    "\n",
    "$$R^{2} = 1- \\frac{SS_r}{SS_t}$$\n",
    "\n",
    "This is nice because it gives us a standardised measure from 0 to 1, which relates to how well the model accounts for the variance in the data. The more variance explained, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = 2 #vary the intercept\n",
    "b1 = 1 #vary the gradient coefficient\n",
    "\n",
    "y_expected = b0 + b1 * x #create a vector of expected values\n",
    "\n",
    "#####################Calculate the R^2 for your chosen values of b0 and b1\n",
    "##Enter code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################BONUS: make this into a function#######################\n",
    "\n",
    "def R2_calc(b0, b1):\n",
    "    ## enter code here\n",
    "    return(r2)\n",
    "\n",
    "print(R2_calc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02674b",
   "metadata": {},
   "source": [
    "## The next step\n",
    "\n",
    "We have seen to metrics for evaluating the fit of a model. Adjusting b0 and b1 give us different values for $R^{2}$ and RMSE. These characterise a **cost function**. There is some value for b0 and b1 where the cost is at it's lowest. We could find this by hand, or we could use optimisation to find it automatically. There are two general approaches when it comes to linear regression. The first is to use the *normal equations*, making use of the properties of vectors and matrices in linear algebra, to analytically find the optimal values of b0 and b1. The other option is to use an iterative algorithm, which tries to find values for b0 and b1 that result in a lower cost than previous values of b0 and b1. This is the basic intuition behind **gradient descent**. Many ML algorithms do not have closed form solutions like linear regression does (e.g. logistic regression), and so we have to rely on iterative algorithms. Gradient descent is also better for large datasets, because the normal equations can take a lot of compute to solve when large matrices are involved (because of calculating matrix inverses).\n",
    "\n",
    "For today, we will use the normal equations. See Chapter 2 and Chapter 4 of the deeplearning book for more insight into how these work.\n",
    "\n",
    "First, we estimate the gradient coefficient using the following equation (where *n* is sample size):\n",
    "\n",
    "$$\\hat{\\beta}_{1} = \\frac{n \\sum x_{j} y_{j} - \\sum x_{j} \\sum y_{j}}{n \\sum x_{j} ^{2} -(\\sum x_{j}) ^{2}} $$\n",
    "\n",
    "Then we estimate the intercept using $\\hat{\\beta}_{1}$ and $\\bar{y}$ and $\\bar{x}$ (i.e. the means of **x** and **y**):\n",
    "\n",
    "$$\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################Finish the code to calculate b0 and b1##########################\n",
    "b1_normeq = \n",
    "b0_normeq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e19e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################Plot the graph with your values fitted\n",
    "\n",
    "min_x = 0\n",
    "max_x = 7\n",
    "\n",
    "xvalues = np.linspace(min_x, max_x, 500) #creates 500 values between 0 and 7\n",
    "\n",
    "\n",
    "yvalues = b0_normeq + b1_normeq * xvalues #create a vector of yvalues\n",
    "\n",
    "plt.plot(xvalues, yvalues, label = 'Line of Best Fit')\n",
    "plt.scatter(x, y, label = 'Data points')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88869f66",
   "metadata": {},
   "source": [
    "If you have written functions to calculate RMSE and $R^{2}$ then calculate those values for your beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a071b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_fitted <- RMSE_calc(b0_normeq, b1_normeq)\n",
    "\n",
    "R2_fitted <- R2_calc(b0_normeq, b1_normeq)\n",
    "\n",
    "print(RMSE_fitted)\n",
    "print(R2_fitted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
